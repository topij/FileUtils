{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FileUtils Tutorial\n",
    "\n",
    "This notebook demonstrates how to use FileUtils for data management in Python data science projects. We'll cover:\n",
    "\n",
    "1. Installation and Setup\n",
    "2. Basic File Operations\n",
    "3. Working with Different File Formats\n",
    "4. Metadata Management\n",
    "5. Azure Storage Integration\n",
    "6. Advanced Configuration\n",
    "\n",
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install FileUtils and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install FileUtils with all features\n",
    "!pip install \"git+https://github.com/topij/FileUtils.git#egg=FileUtils[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from FileUtils import FileUtils, OutputFileType\n",
    "\n",
    "# Initialize FileUtils\n",
    "file_utils = FileUtils()\n",
    "\n",
    "# Create some sample data\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'value': np.random.randn(10),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10)\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic File Operations\n",
    "\n",
    "Let's explore basic file operations with metadata tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data={'sample': df},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"sample_data\"\n",
    ")\n",
    "\n",
    "print(\"Saved files:\", saved_files)\n",
    "print(\"Metadata location:\", metadata)\n",
    "\n",
    "# Load using metadata\n",
    "loaded_data = file_utils.load_from_metadata(metadata)\n",
    "print(\"\\nLoaded data:\")\n",
    "print(loaded_data['sample'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Multiple DataFrames\n",
    "\n",
    "FileUtils can efficiently handle multiple DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple views of the data\n",
    "df_dict = {\n",
    "    'all_data': df,\n",
    "    'filtered': df[df['value'] > 0],\n",
    "    'summary': df.groupby('category').agg({\n",
    "        'value': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "}\n",
    "\n",
    "# Save to Excel with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data=df_dict,\n",
    "    output_filetype=OutputFileType.XLSX,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"multi_sheet_data\"\n",
    ")\n",
    "\n",
    "# Load all sheets\n",
    "loaded_sheets = file_utils.load_excel_sheets(saved_files['multi_sheet_data'])\n",
    "\n",
    "print(\"Excel sheets loaded:\")\n",
    "for name, sheet_df in loaded_sheets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(sheet_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Different File Formats\n",
    "\n",
    "FileUtils supports multiple file formats with automatic handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different formats\n",
    "for format_type in [OutputFileType.CSV, OutputFileType.XLSX, OutputFileType.PARQUET]:\n",
    "    # Save data\n",
    "    saved_files, metadata = file_utils.save_with_metadata(\n",
    "        data={'data': df},\n",
    "        output_filetype=format_type,\n",
    "        output_type=\"processed\",\n",
    "        file_name=f\"format_test_{format_type.value}\"\n",
    "    )\n",
    "    \n",
    "    # Load and verify\n",
    "    loaded_data = file_utils.load_from_metadata(metadata)\n",
    "    print(f\"\\n{format_type.value.upper()} format test:\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Loaded shape: {loaded_data['data'].shape}\")\n",
    "    print(\"Data preserved:\", df.equals(loaded_data['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metadata Management\n",
    "\n",
    "Let's explore the metadata features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save data with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data={\n",
    "        'raw': df,\n",
    "        'processed': df.copy().assign(value=lambda x: x['value'] * 2)\n",
    "    },\n",
    "    output_filetype=OutputFileType.PARQUET,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"metadata_test\"\n",
    ")\n",
    "\n",
    "# Examine metadata contents\n",
    "with open(metadata, 'r') as f:\n",
    "    metadata_content = json.load(f)\n",
    "\n",
    "print(\"Metadata contents:\")\n",
    "print(json.dumps(metadata_content, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Azure Storage Integration\n",
    "\n",
    "To use Azure Storage, you'll need valid credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from FileUtils.core.base import StorageConnectionError\n",
    "\n",
    "# Load credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Try Azure connection\n",
    "try:\n",
    "    connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    if not connection_string:\n",
    "        raise ValueError(\"Azure connection string not found\")\n",
    "        \n",
    "    azure_utils = FileUtils(\n",
    "        storage_type=\"azure\",\n",
    "        connection_string=connection_string\n",
    "    )\n",
    "    \n",
    "    # Save to Azure\n",
    "    saved_files, metadata = azure_utils.save_with_metadata(\n",
    "        data={'test': df},\n",
    "        output_filetype=OutputFileType.PARQUET,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"azure_test\"\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully saved to Azure:\")\n",
    "    print(saved_files)\n",
    "    \n",
    "    # Load from Azure\n",
    "    loaded_data = azure_utils.load_from_metadata(metadata)\n",
    "    print(\"\\nSuccessfully loaded from Azure\")\n",
    "    \n",
    "except (ValueError, StorageConnectionError) as e:\n",
    "    print(f\"Azure setup not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Configuration\n",
    "\n",
    "Let's explore custom configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Create custom config\n",
    "config = {\n",
    "    'csv_delimiter': '|',\n",
    "    'encoding': 'utf-8',\n",
    "    'include_timestamp': True,\n",
    "    'logging_level': 'DEBUG',\n",
    "    'directory_structure': {\n",
    "        'data': ['raw', 'interim', 'processed', 'external'],\n",
    "        'reports': ['figures', 'tables'],\n",
    "        'models': ['trained', 'evaluations']\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = Path('custom_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# Initialize with custom config\n",
    "custom_utils = FileUtils(config_file=config_path)\n",
    "\n",
    "# Test custom configuration\n",
    "saved_files, metadata = custom_utils.save_with_metadata(\n",
    "    data={'test': df},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"custom_config_test\"\n",
    ")\n",
    "\n",
    "# Show custom delimiter in action\n",
    "with open(list(saved_files.values())[0], 'r') as f:\n",
    "    print(\"CSV with custom delimiter:\")\n",
    "    print(f.read())\n",
    "\n",
    "# Clean up\n",
    "config_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "- Check the [Installation Guide](docs/INSTALLATION.md) for detailed setup instructions\n",
    "- See the [Usage Guide](docs/USAGE.md) for more examples and best practices\n",
    "- Refer to [Azure Setup](docs/AZURE_SETUP.md) for cloud storage configuration\n",
    "\n",
    "For issues or suggestions, please visit the GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
