{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FileUtils Tutorial\n",
    "\n",
    "This notebook demonstrates how to use FileUtils for data management in Python data science projects. We'll cover:\n",
    "\n",
    "1. Installation and Setup\n",
    "2. Basic File Operations\n",
    "3. Working with Different File Formats\n",
    "4. Document Handling (NEW!)\n",
    "5. Metadata Management\n",
    "6. Azure Storage Integration\n",
    "7. Advanced Configuration\n",
    "\n",
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install FileUtils and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install FileUtils with all features (including document support)\n",
    "#%pip install \"git+https://github.com/topij/FileUtils.git#egg=FileUtils[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:15:26,900 - FileUtils.core.file_utils - INFO - Project root: /Users/topi/data-science/FileUtils\n",
      "2025-10-18 08:15:26,900 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>1.523030</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     value category\n",
       "0 2024-01-01  0.496714        A\n",
       "1 2024-01-02 -0.138264        A\n",
       "2 2024-01-03  0.647689        B\n",
       "3 2024-01-04  1.523030        B\n",
       "4 2024-01-05 -0.234153        A"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project src directory to path (for local development)\n",
    "# This allows importing FileUtils when running the notebook from the examples directory\n",
    "project_root = Path().resolve().parent\n",
    "src_path = str(project_root / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from FileUtils import FileUtils, OutputFileType\n",
    "\n",
    "# Initialize FileUtils\n",
    "file_utils = FileUtils()\n",
    "\n",
    "# Create some sample data\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'value': np.random.randn(10),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10)\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic File Operations\n",
    "\n",
    "Let's explore basic file operations with metadata tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: {'sample': '/Users/topi/data-science/FileUtils/data/processed/sample_data_20251018_081526_sample.csv'}\n",
      "Metadata location: /Users/topi/data-science/FileUtils/data/processed/sample_data_20251018_081526_metadata.json\n",
      "\n",
      "Loaded data:\n",
      "         date     value category\n",
      "0  2024-01-01  0.496714        A\n",
      "1  2024-01-02 -0.138264        A\n",
      "2  2024-01-03  0.647689        B\n",
      "3  2024-01-04  1.523030        B\n",
      "4  2024-01-05 -0.234153        A\n"
     ]
    }
   ],
   "source": [
    "# Save data with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data={'sample': df},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"sample_data\"\n",
    ")\n",
    "\n",
    "print(\"Saved files:\", saved_files)\n",
    "print(\"Metadata location:\", metadata)\n",
    "\n",
    "# Load using metadata\n",
    "loaded_data = file_utils.load_from_metadata(metadata)\n",
    "print(\"\\nLoaded data:\")\n",
    "print(loaded_data['sample'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Multiple DataFrames\n",
    "\n",
    "FileUtils can efficiently handle multiple DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel sheets loaded:\n",
      "\n",
      "all_data:\n",
      "        date     value category\n",
      "0 2024-01-01  0.496714        A\n",
      "1 2024-01-02 -0.138264        A\n",
      "2 2024-01-03  0.647689        B\n",
      "3 2024-01-04  1.523030        B\n",
      "4 2024-01-05 -0.234153        A\n",
      "\n",
      "filtered:\n",
      "        date     value category\n",
      "0 2024-01-01  0.496714        A\n",
      "1 2024-01-03  0.647689        B\n",
      "2 2024-01-04  1.523030        B\n",
      "3 2024-01-07  1.579213        A\n",
      "4 2024-01-08  0.767435        C\n",
      "\n",
      "summary:\n",
      "   Unnamed: 0 category_  value_mean  value_std  value_count\n",
      "0           0         A    0.293874   0.780639            5\n",
      "1           1         B    1.085359   0.618960            2\n",
      "2           2         C    0.280173   0.658879            3\n"
     ]
    }
   ],
   "source": [
    "# Create multiple views of the data\n",
    "df_dict = {\n",
    "    'all_data': df,\n",
    "    'filtered': df[df['value'] > 0],\n",
    "    'summary': df.groupby('category').agg({\n",
    "        'value': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "}\n",
    "\n",
    "# Save to Excel with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data=df_dict,\n",
    "    output_filetype=OutputFileType.XLSX,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"multi_sheet_data\"\n",
    ")\n",
    "\n",
    "# Get the Excel file path (all sheet names point to the same file)\n",
    "excel_file_path = list(saved_files.values())[0]\n",
    "loaded_sheets = file_utils.load_excel_sheets(excel_file_path)\n",
    "\n",
    "print(\"Excel sheets loaded:\")\n",
    "for name, sheet_df in loaded_sheets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(sheet_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Different File Formats\n",
    "\n",
    "FileUtils supports multiple file formats with automatic handling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Handling (NEW!)\n",
    "\n",
    "FileUtils now supports rich document formats perfect for AI/agentic workflows:\n",
    "\n",
    "- **Markdown (.md)**: Text-based documents with YAML frontmatter support\n",
    "- **Microsoft Word (.docx)**: Structured documents with headings, text, and tables  \n",
    "- **PDF (.pdf)**: Text documents with basic formatting (read-only extraction)\n",
    "\n",
    "Let's explore these new capabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:15:27,072 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/ai_analysis_report_20251018_081527.md\n",
      "Markdown document saved to: /Users/topi/data-science/FileUtils/data/processed/ai_analysis_report_20251018_081527.md\n",
      "\n",
      "Loaded markdown content:\n",
      "# AI Analysis Report\n",
      "\n",
      "## Executive Summary\n",
      "This report analyzes the performance of our AI models using FileUtils.\n",
      "\n",
      "## Key Findings\n",
      "- Model accuracy: 95.2%\n",
      "- Processing time: 2.3 seconds\n",
      "- User satisfa...\n"
     ]
    }
   ],
   "source": [
    "# Markdown Document Example\n",
    "markdown_content = \"\"\"# AI Analysis Report\n",
    "\n",
    "## Executive Summary\n",
    "This report analyzes the performance of our AI models using FileUtils.\n",
    "\n",
    "## Key Findings\n",
    "- Model accuracy: 95.2%\n",
    "- Processing time: 2.3 seconds\n",
    "- User satisfaction: 4.8/5\n",
    "\n",
    "## Recommendations\n",
    "1. Implement additional training data\n",
    "2. Optimize inference pipeline\n",
    "3. Add real-time monitoring\n",
    "\"\"\"\n",
    "\n",
    "# Save simple markdown\n",
    "saved_path, _ = file_utils.save_document_to_storage(\n",
    "    content=markdown_content,\n",
    "    output_filetype=OutputFileType.MARKDOWN,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"ai_analysis_report\"\n",
    ")\n",
    "\n",
    "print(\"Markdown document saved to:\", saved_path)\n",
    "\n",
    "# Load markdown\n",
    "loaded_content = file_utils.load_document_from_storage(\n",
    "    file_path=\"ai_analysis_report.md\",\n",
    "    input_type=\"processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded markdown content:\")\n",
    "print(loaded_content[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:15:27,081 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/reports/2024/model_performance_report_20251018_081527.md\n",
      "Structured markdown saved to: /Users/topi/data-science/FileUtils/data/processed/reports/2024/model_performance_report_20251018_081527.md\n",
      "\n",
      "Report by AI Team with 0.95 confidence\n",
      "Model: GPT-4\n",
      "Content preview: # AI Model Performance Report\n",
      "\n",
      "## Model Metrics\n",
      "\n",
      "| Model | Accuracy | Precision | Recall | F1-Score ...\n"
     ]
    }
   ],
   "source": [
    "# Markdown with YAML Frontmatter Example\n",
    "structured_content = {\n",
    "    \"frontmatter\": {\n",
    "        \"title\": \"AI Model Performance Report\",\n",
    "        \"author\": \"AI Team\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"tags\": [\"AI\", \"Performance\", \"Analysis\"],\n",
    "        \"confidence\": 0.95,\n",
    "        \"model\": \"GPT-4\"\n",
    "    },\n",
    "    \"body\": \"\"\"# AI Model Performance Report\n",
    "\n",
    "## Model Metrics\n",
    "\n",
    "| Model | Accuracy | Precision | Recall | F1-Score |\n",
    "|-------|----------|-----------|--------|----------|\n",
    "| Model A | 94.2% | 93.8% | 94.5% | 94.1% |\n",
    "| Model B | 95.7% | 95.2% | 96.1% | 95.6% |\n",
    "| Model C | 96.1% | 95.8% | 96.4% | 96.1% |\n",
    "\n",
    "## Analysis\n",
    "Model C shows the best overall performance across all metrics.\n",
    "\n",
    "## Recommendations\n",
    "1. Deploy Model C to production\n",
    "2. Monitor performance metrics\n",
    "3. Schedule retraining cycle\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Save structured markdown\n",
    "saved_path, _ = file_utils.save_document_to_storage(\n",
    "    content=structured_content,\n",
    "    output_filetype=OutputFileType.MARKDOWN,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"model_performance_report\",\n",
    "    sub_path=\"reports/2024\"\n",
    ")\n",
    "\n",
    "print(\"Structured markdown saved to:\", saved_path)\n",
    "\n",
    "# Load structured markdown\n",
    "loaded_content = file_utils.load_document_from_storage(\n",
    "    file_path=\"model_performance_report.md\",\n",
    "    input_type=\"processed\",\n",
    "    sub_path=\"reports/2024\"\n",
    ")\n",
    "\n",
    "# Access frontmatter and body separately\n",
    "if isinstance(loaded_content, dict):\n",
    "    metadata = loaded_content[\"frontmatter\"]\n",
    "    content = loaded_content[\"body\"]\n",
    "    print(f\"\\nReport by {metadata['author']} with {metadata['confidence']} confidence\")\n",
    "    print(f\"Model: {metadata['model']}\")\n",
    "    print(f\"Content preview: {content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:15:27,141 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/simple_document_20251018_081527.docx\n",
      "DOCX document saved to: /Users/topi/data-science/FileUtils/data/processed/simple_document_20251018_081527.docx\n",
      "\n",
      "Loaded DOCX content:\n",
      "This is a test document for DOCX format created with FileUtils.\n"
     ]
    }
   ],
   "source": [
    "# DOCX Document Example (requires python-docx)\n",
    "try:\n",
    "    # Simple DOCX document\n",
    "    docx_content = \"This is a test document for DOCX format created with FileUtils.\"\n",
    "    \n",
    "    saved_path, _ = file_utils.save_document_to_storage(\n",
    "        content=docx_content,\n",
    "        output_filetype=OutputFileType.DOCX,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"simple_document\"\n",
    "    )\n",
    "    \n",
    "    print(\"DOCX document saved to:\", saved_path)\n",
    "    \n",
    "    # Load DOCX (extracts text content)\n",
    "    loaded_content = file_utils.load_document_from_storage(\n",
    "        file_path=\"simple_document.docx\",\n",
    "        input_type=\"processed\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLoaded DOCX content:\")\n",
    "    print(loaded_content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DOCX functionality requires python-docx: {e}\")\n",
    "    print(\"Install with: pip install python-docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:15:28,658 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/simple_pdf_20251018_081527.pdf\n",
      "PDF document saved to: /Users/topi/data-science/FileUtils/data/processed/simple_pdf_20251018_081527.pdf\n",
      "\n",
      "Loaded PDF content:\n",
      "This is a test document for PDF format created with FileUtils.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PDF Document Example (requires PyMuPDF)\n",
    "try:\n",
    "    # Simple PDF document\n",
    "    pdf_content = \"This is a test document for PDF format created with FileUtils.\"\n",
    "    \n",
    "    saved_path, _ = file_utils.save_document_to_storage(\n",
    "        content=pdf_content,\n",
    "        output_filetype=OutputFileType.PDF,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"simple_pdf\"\n",
    "    )\n",
    "    \n",
    "    print(\"PDF document saved to:\", saved_path)\n",
    "    \n",
    "    # Load PDF (extracts text content)\n",
    "    loaded_content = file_utils.load_document_from_storage(\n",
    "        file_path=\"simple_pdf.pdf\",\n",
    "        input_type=\"processed\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLoaded PDF content:\")\n",
    "    print(loaded_content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PDF functionality requires PyMuPDF: {e}\")\n",
    "    print(\"Install with: pip install PyMuPDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV format test:\n",
      "Original shape: (10, 3)\n",
      "Loaded shape: (10, 3)\n",
      "Data preserved: False\n",
      "\n",
      "XLSX format test:\n",
      "Original shape: (10, 3)\n",
      "Loaded shape: (10, 3)\n",
      "Data preserved: False\n",
      "\n",
      "PARQUET format test:\n",
      "Original shape: (10, 3)\n",
      "Loaded shape: (10, 3)\n",
      "Data preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Test different formats\n",
    "for format_type in [OutputFileType.CSV, OutputFileType.XLSX, OutputFileType.PARQUET]:\n",
    "    # Save data\n",
    "    saved_files, metadata = file_utils.save_with_metadata(\n",
    "        data={'data': df},\n",
    "        output_filetype=format_type,\n",
    "        output_type=\"processed\",\n",
    "        file_name=f\"format_test_{format_type.value}\"\n",
    "    )\n",
    "    \n",
    "    # Load and verify\n",
    "    loaded_data = file_utils.load_from_metadata(metadata)\n",
    "    print(f\"\\n{format_type.value.upper()} format test:\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Loaded shape: {loaded_data['data'].shape}\")\n",
    "    print(\"Data preserved:\", df.equals(loaded_data['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata Management\n",
    "\n",
    "Let's explore the metadata features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata contents:\n",
      "{\n",
      "  \"timestamp\": \"2025-10-18T08:15:34.321388\",\n",
      "  \"files\": {\n",
      "    \"raw\": {\n",
      "      \"path\": \"/Users/topi/data-science/FileUtils/data/processed/metadata_test_20251018_081534_raw.parquet\",\n",
      "      \"format\": \"parquet\"\n",
      "    },\n",
      "    \"processed\": {\n",
      "      \"path\": \"/Users/topi/data-science/FileUtils/data/processed/metadata_test_20251018_081534_processed.parquet\",\n",
      "      \"format\": \"parquet\"\n",
      "    }\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"directory_structure\": {\n",
      "      \"data\": [\n",
      "        \"raw\",\n",
      "        \"processed\"\n",
      "      ]\n",
      "    },\n",
      "    \"csv_delimiter\": \";\",\n",
      "    \"encoding\": \"utf-8\",\n",
      "    \"quoting\": 0,\n",
      "    \"include_timestamp\": true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save data with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data={\n",
    "        'raw': df,\n",
    "        'processed': df.copy().assign(value=lambda x: x['value'] * 2)\n",
    "    },\n",
    "    output_filetype=OutputFileType.PARQUET,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"metadata_test\"\n",
    ")\n",
    "\n",
    "# Examine metadata contents\n",
    "with open(metadata, 'r') as f:\n",
    "    metadata_content = json.load(f)\n",
    "\n",
    "print(\"Metadata contents:\")\n",
    "print(json.dumps(metadata_content, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Azure Storage Integration\n",
    "\n",
    "To use Azure Storage, you'll need valid credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure setup not available: Azure connection string not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from FileUtils.core.base import StorageConnectionError\n",
    "\n",
    "# Load credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Try Azure connection\n",
    "try:\n",
    "    connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    if not connection_string:\n",
    "        raise ValueError(\"Azure connection string not found\")\n",
    "        \n",
    "    azure_utils = FileUtils(\n",
    "        storage_type=\"azure\",\n",
    "        connection_string=connection_string\n",
    "    )\n",
    "    \n",
    "    # Save to Azure\n",
    "    saved_files, metadata = azure_utils.save_with_metadata(\n",
    "        data={'test': df},\n",
    "        output_filetype=OutputFileType.PARQUET,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"azure_test\"\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully saved to Azure:\")\n",
    "    print(saved_files)\n",
    "    \n",
    "    # Load from Azure\n",
    "    loaded_data = azure_utils.load_from_metadata(metadata)\n",
    "    print(\"\\nSuccessfully loaded from Azure\")\n",
    "    \n",
    "except (ValueError, StorageConnectionError) as e:\n",
    "    print(f\"Azure setup not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Configuration\n",
    "\n",
    "Let's explore custom configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-18 08:15:34,351 - FileUtils.core.file_utils - INFO - Project root: /Users/topi/data-science/FileUtils\n",
      "2025-10-18 08:15:34,351 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "CSV with custom delimiter:\n",
      "date|value|category\n",
      "2024-01-01|0.4967141530112327|A\n",
      "2024-01-02|-0.13826430117118466|A\n",
      "2024-01-03|0.6476885381006925|B\n",
      "2024-01-04|1.5230298564080254|B\n",
      "2024-01-05|-0.23415337472333597|A\n",
      "2024-01-06|-0.23413695694918055|A\n",
      "2024-01-07|1.5792128155073915|A\n",
      "2024-01-08|0.7674347291529088|C\n",
      "2024-01-09|-0.4694743859349521|C\n",
      "2024-01-10|0.5425600435859647|C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Create custom config\n",
    "config = {\n",
    "    'csv_delimiter': '|',\n",
    "    'encoding': 'utf-8',\n",
    "    'include_timestamp': True,\n",
    "    'logging_level': 'DEBUG',\n",
    "    'directory_structure': {\n",
    "        'data': ['raw', 'interim', 'processed', 'external'],\n",
    "        'reports': ['figures', 'tables'],\n",
    "        'models': ['trained', 'evaluations']\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = Path('custom_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# Initialize with custom config\n",
    "custom_utils = FileUtils(config_file=config_path)\n",
    "\n",
    "# Test custom configuration\n",
    "saved_files, metadata = custom_utils.save_with_metadata(\n",
    "    data={'test': df},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"custom_config_test\"\n",
    ")\n",
    "\n",
    "# Show custom delimiter in action\n",
    "with open(list(saved_files.values())[0], 'r') as f:\n",
    "    print(\"CSV with custom delimiter:\")\n",
    "    print(f.read())\n",
    "\n",
    "# Clean up\n",
    "config_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "- Check the [Installation Guide](docs/INSTALLATION.md) for detailed setup instructions\n",
    "- See the [Usage Guide](docs/USAGE.md) for more examples and best practices\n",
    "- Explore the [Document Types Guide](docs/DOCUMENT_TYPES.md) for comprehensive document handling\n",
    "- Refer to [Azure Setup](docs/AZURE_SETUP.md) for cloud storage configuration\n",
    "\n",
    "For issues or suggestions, please visit the GitHub repository.\n",
    "\n",
    "For issues or suggestions, please visit the GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fileutils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
