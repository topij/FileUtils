{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FileUtils Tutorial\n",
    "\n",
    "This notebook demonstrates how to use FileUtils for data management in Python data science projects. We'll cover:\n",
    "\n",
    "1. Installation and Setup\n",
    "2. Basic File Operations\n",
    "3. Working with Different File Formats\n",
    "4. Document Handling (NEW!)\n",
    "5. Metadata Management\n",
    "6. Azure Storage Integration\n",
    "7. Advanced Configuration\n",
    "\n",
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install FileUtils and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install FileUtils with all features (including document support)\n",
    "#%pip install \"git+https://github.com/topij/FileUtils.git#egg=FileUtils[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,436 - FileUtils.core.file_utils - INFO - Project root: /Users/topi/data-science/FileUtils\n",
      "2025-10-19 01:12:22,437 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>1.523030</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     value category\n",
       "0 2024-01-01  0.496714        A\n",
       "1 2024-01-02 -0.138264        A\n",
       "2 2024-01-03  0.647689        B\n",
       "3 2024-01-04  1.523030        B\n",
       "4 2024-01-05 -0.234153        A"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project src directory to path (for local development)\n",
    "# This allows importing FileUtils when running the notebook from the examples directory\n",
    "project_root = Path().resolve().parent\n",
    "src_path = str(project_root / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from FileUtils import FileUtils, OutputFileType\n",
    "\n",
    "# Initialize FileUtils\n",
    "file_utils = FileUtils()\n",
    "\n",
    "# Create some sample data\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'value': np.random.randn(10),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10)\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic File Operations\n",
    "\n",
    "Let's explore basic file operations with metadata tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: {'sample': '/Users/topi/data-science/FileUtils/data/processed/sample_data_20251019_011222_sample.csv'}\n",
      "Metadata location: /Users/topi/data-science/FileUtils/data/processed/sample_data_20251019_011222_metadata.json\n",
      "\n",
      "Loaded data:\n",
      "         date     value category\n",
      "0  2024-01-01  0.496714        A\n",
      "1  2024-01-02 -0.138264        A\n",
      "2  2024-01-03  0.647689        B\n",
      "3  2024-01-04  1.523030        B\n",
      "4  2024-01-05 -0.234153        A\n"
     ]
    }
   ],
   "source": [
    "# Save data with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data={'sample': df},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"sample_data\"\n",
    ")\n",
    "\n",
    "print(\"Saved files:\", saved_files)\n",
    "print(\"Metadata location:\", metadata)\n",
    "\n",
    "# Load using metadata\n",
    "loaded_data = file_utils.load_from_metadata(metadata)\n",
    "print(\"\\nLoaded data:\")\n",
    "print(loaded_data['sample'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Multiple DataFrames\n",
    "\n",
    "FileUtils can efficiently handle multiple DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel sheets loaded:\n",
      "\n",
      "all_data:\n",
      "        date     value category\n",
      "0 2024-01-01  0.496714        A\n",
      "1 2024-01-02 -0.138264        A\n",
      "2 2024-01-03  0.647689        B\n",
      "3 2024-01-04  1.523030        B\n",
      "4 2024-01-05 -0.234153        A\n",
      "\n",
      "filtered:\n",
      "        date     value category\n",
      "0 2024-01-01  0.496714        A\n",
      "1 2024-01-03  0.647689        B\n",
      "2 2024-01-04  1.523030        B\n",
      "3 2024-01-07  1.579213        A\n",
      "4 2024-01-08  0.767435        C\n",
      "\n",
      "summary:\n",
      "   Unnamed: 0 category_  value_mean  value_std  value_count\n",
      "0           0         A    0.293874   0.780639            5\n",
      "1           1         B    1.085359   0.618960            2\n",
      "2           2         C    0.280173   0.658879            3\n"
     ]
    }
   ],
   "source": [
    "# Create multiple views of the data\n",
    "df_dict = {\n",
    "    'all_data': df,\n",
    "    'filtered': df[df['value'] > 0],\n",
    "    'summary': df.groupby('category').agg({\n",
    "        'value': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "}\n",
    "\n",
    "# Save to Excel with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data=df_dict,\n",
    "    output_filetype=OutputFileType.XLSX,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"multi_sheet_data\"\n",
    ")\n",
    "\n",
    "# Get the Excel file path (all sheet names point to the same file)\n",
    "excel_file_path = list(saved_files.values())[0]\n",
    "loaded_sheets = file_utils.load_excel_sheets(excel_file_path)\n",
    "\n",
    "print(\"Excel sheets loaded:\")\n",
    "for name, sheet_df in loaded_sheets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(sheet_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Different File Formats\n",
    "\n",
    "FileUtils supports multiple file formats with automatic handling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Format\n",
    "\n",
    "CSV files are perfect for simple tabular data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,593 - FileUtils.core.file_utils - INFO - Data saved successfully: {'sales_data': '/Users/topi/data-science/FileUtils/data/processed/sales_report_20251019_011222.csv'}\n",
      "CSV saved to: {'sales_data': '/Users/topi/data-science/FileUtils/data/processed/sales_report_20251019_011222.csv'}\n",
      "\n",
      "CSV data loaded:\n",
      "         date     value category\n",
      "0  2024-01-01  0.496714        A\n",
      "1  2024-01-02 -0.138264        A\n",
      "2  2024-01-03  0.647689        B\n",
      "3  2024-01-04  1.523030        B\n",
      "4  2024-01-05 -0.234153        A\n"
     ]
    }
   ],
   "source": [
    "# CSV Example\n",
    "csv_data = df.copy()\n",
    "\n",
    "# Save as CSV\n",
    "saved_files, metadata = file_utils.save_data_to_storage(\n",
    "    data={'sales_data': csv_data},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"sales_report\"\n",
    ")\n",
    "\n",
    "print(\"CSV saved to:\", saved_files)\n",
    "\n",
    "# Load CSV data using load_single_file\n",
    "# For single DataFrame, the file is saved as base_name.csv (without the key name)\n",
    "loaded_csv = file_utils.load_single_file(\n",
    "    file_path=\"sales_report.csv\",\n",
    "    input_type=\"processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nCSV data loaded:\")\n",
    "print(loaded_csv.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Format\n",
    "\n",
    "Excel files support multiple sheets and complex formatting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,609 - FileUtils.core.file_utils - INFO - Data saved successfully: {'summary': '/Users/topi/data-science/FileUtils/data/processed/comprehensive_report_20251019_011222.xlsx', 'raw_data': '/Users/topi/data-science/FileUtils/data/processed/comprehensive_report_20251019_011222.xlsx', 'filtered': '/Users/topi/data-science/FileUtils/data/processed/comprehensive_report_20251019_011222.xlsx'}\n",
      "Excel file saved to: /Users/topi/data-science/FileUtils/data/processed/comprehensive_report_20251019_011222.xlsx\n",
      "\n",
      "Excel sheets loaded: ['summary', 'raw_data', 'filtered']\n",
      "\n",
      "Summary sheet:\n",
      "   Unnamed: 0 category_  value_mean  value_std  value_count\n",
      "0           0         A    0.293874   0.780639            5\n",
      "1           1         B    1.085359   0.618960            2\n",
      "2           2         C    0.280173   0.658879            3\n"
     ]
    }
   ],
   "source": [
    "# Excel Example with Multiple Sheets\n",
    "excel_data = {\n",
    "    'summary': df.groupby('category').agg({\n",
    "        'value': ['mean', 'std', 'count']\n",
    "    }).reset_index(),\n",
    "    'raw_data': df,\n",
    "    'filtered': df[df['value'] > 0]\n",
    "}\n",
    "\n",
    "# Save as Excel\n",
    "saved_files, metadata = file_utils.save_data_to_storage(\n",
    "    data=excel_data,\n",
    "    output_filetype=OutputFileType.XLSX,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"comprehensive_report\"\n",
    ")\n",
    "\n",
    "print(\"Excel file saved to:\", list(saved_files.values())[0])\n",
    "\n",
    "# Load Excel data (all sheets)\n",
    "excel_file_path = list(saved_files.values())[0]\n",
    "loaded_excel = file_utils.load_excel_sheets(excel_file_path)\n",
    "\n",
    "print(f\"\\nExcel sheets loaded: {list(loaded_excel.keys())}\")\n",
    "print(\"\\nSummary sheet:\")\n",
    "print(loaded_excel['summary'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Format\n",
    "\n",
    "Parquet files offer efficient storage for large datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,686 - FileUtils.core.file_utils - INFO - Data saved successfully: {'large_dataset': '/Users/topi/data-science/FileUtils/data/processed/large_dataset_20251019_011222.parquet'}\n",
      "Parquet file saved to: {'large_dataset': '/Users/topi/data-science/FileUtils/data/processed/large_dataset_20251019_011222.parquet'}\n",
      "\n",
      "Parquet data loaded: (1000, 5)\n",
      "Sample data:\n",
      "   id           timestamp     value category      score\n",
      "0   0 2024-01-01 00:00:00 -0.428046        D  74.807318\n",
      "1   1 2024-01-01 01:00:00 -0.742407        D  81.176979\n",
      "2   2 2024-01-01 02:00:00 -0.703344        A  65.647861\n",
      "3   3 2024-01-01 03:00:00 -2.139621        C  12.809575\n",
      "4   4 2024-01-01 04:00:00 -0.629475        C  33.826751\n"
     ]
    }
   ],
   "source": [
    "# Parquet Example\n",
    "# Create a larger dataset for Parquet demonstration\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(1000),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=1000, freq='h'),  # Fixed deprecation warning\n",
    "    'value': np.random.randn(1000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 1000),\n",
    "    'score': np.random.uniform(0, 100, 1000)\n",
    "})\n",
    "\n",
    "# Save as Parquet\n",
    "saved_files, metadata = file_utils.save_data_to_storage(\n",
    "    data={'large_dataset': large_df},\n",
    "    output_filetype=OutputFileType.PARQUET,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"large_dataset\"\n",
    ")\n",
    "\n",
    "print(\"Parquet file saved to:\", saved_files)\n",
    "\n",
    "# Load Parquet data using load_single_file\n",
    "# For single DataFrame, the file is saved as base_name.parquet (without the key name)\n",
    "loaded_parquet = file_utils.load_single_file(\n",
    "    file_path=\"large_dataset.parquet\",\n",
    "    input_type=\"processed\"\n",
    ")\n",
    "\n",
    "print(f\"\\nParquet data loaded: {loaded_parquet.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(loaded_parquet.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Format\n",
    "\n",
    "JSON files are great for structured data and configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,768 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/analysis_results_20251019_011222.json\n",
      "JSON file saved to: /Users/topi/data-science/FileUtils/data/processed/analysis_results_20251019_011222.json\n",
      "\n",
      "JSON data loaded:\n",
      "Metadata: {'version': '1.0', 'created': '2024-01-15', 'author': 'Data Team'}\n",
      "Summary stats: {'total_records': 10, 'categories': {'A': 5, 'C': 3, 'B': 2}, 'avg_value': 0.44806111169875623, 'date_range': {'start': '2024-01-01 00:00:00', 'end': '2024-01-10 00:00:00'}}\n",
      "Data records: 10\n",
      "Sample data: {'date': '2024-01-01T00:00:00', 'value': 0.4967141530112327, 'category': 'A'}\n"
     ]
    }
   ],
   "source": [
    "# JSON Example\n",
    "# Create structured data for JSON\n",
    "json_data = {\n",
    "    'metadata': {\n",
    "        'version': '1.0',\n",
    "        'created': '2024-01-15',\n",
    "        'author': 'Data Team'\n",
    "    },\n",
    "    'summary_stats': {\n",
    "        'total_records': len(df),\n",
    "        'categories': df['category'].value_counts().to_dict(),\n",
    "        'avg_value': float(df['value'].mean()),\n",
    "        'date_range': {\n",
    "            'start': str(df['date'].min()),\n",
    "            'end': str(df['date'].max())\n",
    "        }\n",
    "    },\n",
    "    'data': df.to_dict('records')  # Pandas Timestamps automatically converted to ISO format\n",
    "}\n",
    "\n",
    "# Save as JSON using document method (since it's not a DataFrame)\n",
    "saved_path, _ = file_utils.save_document_to_storage(\n",
    "    content=json_data,\n",
    "    output_filetype=OutputFileType.JSON,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"analysis_results\"\n",
    ")\n",
    "\n",
    "print(\"JSON file saved to:\", saved_path)\n",
    "\n",
    "# Load JSON data using load_json method\n",
    "loaded_json = file_utils.load_json(\n",
    "    file_path=\"analysis_results.json\",\n",
    "    input_type=\"processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nJSON data loaded:\")\n",
    "print(\"Metadata:\", loaded_json['metadata'])\n",
    "print(\"Summary stats:\", loaded_json['summary_stats'])\n",
    "print(f\"Data records: {len(loaded_json['data'])}\")\n",
    "print(\"Sample data:\", loaded_json['data'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML Format\n",
    "\n",
    "YAML files are perfect for configuration and human-readable data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,776 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/pipeline_config_20251019_011222.yaml\n",
      "YAML file saved to: /Users/topi/data-science/FileUtils/data/processed/pipeline_config_20251019_011222.yaml\n",
      "\n",
      "YAML data loaded:\n",
      "Project: Data Analysis Pipeline\n",
      "Data sources: ['primary', 'secondary']\n",
      "Processing config: {'batch_size': 1000, 'parallel_workers': 4, 'retry_attempts': 3, 'timeout_seconds': 300}\n"
     ]
    }
   ],
   "source": [
    "# YAML Example\n",
    "# Create configuration data for YAML\n",
    "yaml_config = {\n",
    "    'project': {\n",
    "        'name': 'Data Analysis Pipeline',\n",
    "        'version': '2.1.0',\n",
    "        'description': 'Automated data processing and analysis'\n",
    "    },\n",
    "    'data_sources': {\n",
    "        'primary': {\n",
    "            'type': 'database',\n",
    "            'connection': 'postgresql://localhost:5432/analytics',\n",
    "            'tables': ['users', 'transactions', 'products']\n",
    "        },\n",
    "        'secondary': {\n",
    "            'type': 'api',\n",
    "            'url': 'https://api.example.com/data',\n",
    "            'auth': 'bearer_token'\n",
    "        }\n",
    "    },\n",
    "    'processing': {\n",
    "        'batch_size': 1000,\n",
    "        'parallel_workers': 4,\n",
    "        'retry_attempts': 3,\n",
    "        'timeout_seconds': 300\n",
    "    },\n",
    "    'output': {\n",
    "        'formats': ['csv', 'parquet', 'json'],\n",
    "        'compression': 'gzip',\n",
    "        'partition_by': ['date', 'category']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as YAML using document method (since it's not a DataFrame)\n",
    "saved_path, _ = file_utils.save_document_to_storage(\n",
    "    content=yaml_config,\n",
    "    output_filetype=OutputFileType.YAML,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"pipeline_config\"\n",
    ")\n",
    "\n",
    "print(\"YAML file saved to:\", saved_path)\n",
    "\n",
    "# Load YAML data using load_yaml method\n",
    "loaded_yaml = file_utils.load_yaml(\n",
    "    file_path=\"pipeline_config.yaml\",\n",
    "    input_type=\"processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nYAML data loaded:\")\n",
    "print(\"Project:\", loaded_yaml['project']['name'])\n",
    "print(\"Data sources:\", list(loaded_yaml['data_sources'].keys()))\n",
    "print(\"Processing config:\", loaded_yaml['processing'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Comparison\n",
    "\n",
    "Each format has its strengths:\n",
    "\n",
    "| Format | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **CSV** | Simple tabular data | Human-readable, universal support | No data types, large files |\n",
    "| **Excel** | Multi-sheet reports | Rich formatting, multiple sheets | Proprietary, slower for large data |\n",
    "| **Parquet** | Large datasets | Fast, compressed, columnar | Binary format, requires special tools |\n",
    "| **JSON** | Structured data | Flexible schema, web-friendly | Verbose, slower parsing |\n",
    "| **YAML** | Configuration | Human-readable, clean syntax | Sensitive to indentation |\n",
    "\n",
    "**Pro Tip**: Use `save_with_metadata()` to automatically track file information and relationships across formats!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Handling (NEW!)\n",
    "\n",
    "FileUtils now supports rich document formats perfect for AI/agentic workflows:\n",
    "\n",
    "- **Markdown (.md)**: Text-based documents with YAML frontmatter support\n",
    "- **Microsoft Word (.docx)**: Structured documents with headings, text, and tables  \n",
    "- **PDF (.pdf)**: Text documents with basic formatting (read-only extraction)\n",
    "\n",
    "Let's explore these new capabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,785 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/ai_analysis_report_20251019_011222.md\n",
      "Markdown document saved to: /Users/topi/data-science/FileUtils/data/processed/ai_analysis_report_20251019_011222.md\n",
      "\n",
      "Loaded markdown content:\n",
      "# AI Analysis Report\n",
      "\n",
      "## Executive Summary\n",
      "This report analyzes the performance of our AI models using FileUtils.\n",
      "\n",
      "## Key Findings\n",
      "- Model accuracy: 95.2%\n",
      "- Processing time: 2.3 seconds\n",
      "- User satisfa...\n"
     ]
    }
   ],
   "source": [
    "# Markdown Document Example\n",
    "markdown_content = \"\"\"# AI Analysis Report\n",
    "\n",
    "## Executive Summary\n",
    "This report analyzes the performance of our AI models using FileUtils.\n",
    "\n",
    "## Key Findings\n",
    "- Model accuracy: 95.2%\n",
    "- Processing time: 2.3 seconds\n",
    "- User satisfaction: 4.8/5\n",
    "\n",
    "## Recommendations\n",
    "1. Implement additional training data\n",
    "2. Optimize inference pipeline\n",
    "3. Add real-time monitoring\n",
    "\"\"\"\n",
    "\n",
    "# Save simple markdown\n",
    "saved_path, _ = file_utils.save_document_to_storage(\n",
    "    content=markdown_content,\n",
    "    output_filetype=OutputFileType.MARKDOWN,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"ai_analysis_report\"\n",
    ")\n",
    "\n",
    "print(\"Markdown document saved to:\", saved_path)\n",
    "\n",
    "# Load markdown\n",
    "loaded_content = file_utils.load_document_from_storage(\n",
    "    file_path=\"ai_analysis_report.md\",\n",
    "    input_type=\"processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded markdown content:\")\n",
    "print(loaded_content[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,791 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/reports/2024/model_performance_report_20251019_011222.md\n",
      "Structured markdown saved to: /Users/topi/data-science/FileUtils/data/processed/reports/2024/model_performance_report_20251019_011222.md\n",
      "\n",
      "Report by AI Team with 0.95 confidence\n",
      "Model: GPT-4\n",
      "Content preview: # AI Model Performance Report\n",
      "\n",
      "## Model Metrics\n",
      "\n",
      "| Model | Accuracy | Precision | Recall | F1-Score ...\n"
     ]
    }
   ],
   "source": [
    "# Markdown with YAML Frontmatter Example\n",
    "structured_content = {\n",
    "    \"frontmatter\": {\n",
    "        \"title\": \"AI Model Performance Report\",\n",
    "        \"author\": \"AI Team\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"tags\": [\"AI\", \"Performance\", \"Analysis\"],\n",
    "        \"confidence\": 0.95,\n",
    "        \"model\": \"GPT-4\"\n",
    "    },\n",
    "    \"body\": \"\"\"# AI Model Performance Report\n",
    "\n",
    "## Model Metrics\n",
    "\n",
    "| Model | Accuracy | Precision | Recall | F1-Score |\n",
    "|-------|----------|-----------|--------|----------|\n",
    "| Model A | 94.2% | 93.8% | 94.5% | 94.1% |\n",
    "| Model B | 95.7% | 95.2% | 96.1% | 95.6% |\n",
    "| Model C | 96.1% | 95.8% | 96.4% | 96.1% |\n",
    "\n",
    "## Analysis\n",
    "Model C shows the best overall performance across all metrics.\n",
    "\n",
    "## Recommendations\n",
    "1. Deploy Model C to production\n",
    "2. Monitor performance metrics\n",
    "3. Schedule retraining cycle\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Save structured markdown\n",
    "saved_path, _ = file_utils.save_document_to_storage(\n",
    "    content=structured_content,\n",
    "    output_filetype=OutputFileType.MARKDOWN,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"model_performance_report\",\n",
    "    sub_path=\"reports/2024\"\n",
    ")\n",
    "\n",
    "print(\"Structured markdown saved to:\", saved_path)\n",
    "\n",
    "# Load structured markdown\n",
    "loaded_content = file_utils.load_document_from_storage(\n",
    "    file_path=\"model_performance_report.md\",\n",
    "    input_type=\"processed\",\n",
    "    sub_path=\"reports/2024\"\n",
    ")\n",
    "\n",
    "# Access frontmatter and body separately\n",
    "if isinstance(loaded_content, dict):\n",
    "    metadata = loaded_content[\"frontmatter\"]\n",
    "    content = loaded_content[\"body\"]\n",
    "    print(f\"\\nReport by {metadata['author']} with {metadata['confidence']} confidence\")\n",
    "    print(f\"Model: {metadata['model']}\")\n",
    "    print(f\"Content preview: {content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,838 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/simple_document_20251019_011222.docx\n",
      "DOCX document saved to: /Users/topi/data-science/FileUtils/data/processed/simple_document_20251019_011222.docx\n",
      "\n",
      "Loaded DOCX content:\n",
      "This is a test document for DOCX format created with FileUtils.\n"
     ]
    }
   ],
   "source": [
    "# DOCX Document Example (requires python-docx)\n",
    "try:\n",
    "    # Simple DOCX document\n",
    "    docx_content = \"This is a test document for DOCX format created with FileUtils.\"\n",
    "    \n",
    "    saved_path, _ = file_utils.save_document_to_storage(\n",
    "        content=docx_content,\n",
    "        output_filetype=OutputFileType.DOCX,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"simple_document\"\n",
    "    )\n",
    "    \n",
    "    print(\"DOCX document saved to:\", saved_path)\n",
    "    \n",
    "    # Load DOCX (extracts text content)\n",
    "    loaded_content = file_utils.load_document_from_storage(\n",
    "        file_path=\"simple_document.docx\",\n",
    "        input_type=\"processed\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLoaded DOCX content:\")\n",
    "    print(loaded_content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DOCX functionality requires python-docx: {e}\")\n",
    "    print(\"Install with: pip install python-docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:22,906 - FileUtils.core.file_utils - INFO - Document saved successfully: /Users/topi/data-science/FileUtils/data/processed/simple_pdf_20251019_011222.pdf\n",
      "PDF document saved to: /Users/topi/data-science/FileUtils/data/processed/simple_pdf_20251019_011222.pdf\n",
      "\n",
      "Loaded PDF content:\n",
      "This is a test document for PDF format created with FileUtils.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PDF Document Example (requires PyMuPDF)\n",
    "try:\n",
    "    # Simple PDF document\n",
    "    pdf_content = \"This is a test document for PDF format created with FileUtils.\"\n",
    "    \n",
    "    saved_path, _ = file_utils.save_document_to_storage(\n",
    "        content=pdf_content,\n",
    "        output_filetype=OutputFileType.PDF,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"simple_pdf\"\n",
    "    )\n",
    "    \n",
    "    print(\"PDF document saved to:\", saved_path)\n",
    "    \n",
    "    # Load PDF (extracts text content)\n",
    "    loaded_content = file_utils.load_document_from_storage(\n",
    "        file_path=\"simple_pdf.pdf\",\n",
    "        input_type=\"processed\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLoaded PDF content:\")\n",
    "    print(loaded_content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PDF functionality requires PyMuPDF: {e}\")\n",
    "    print(\"Install with: pip install PyMuPDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV format test:\n",
      "Original shape: (10, 3)\n",
      "Loaded shape: (10, 3)\n",
      "Data preserved: False\n",
      "\n",
      "XLSX format test:\n",
      "Original shape: (10, 3)\n",
      "Loaded shape: (10, 3)\n",
      "Data preserved: False\n",
      "\n",
      "PARQUET format test:\n",
      "Original shape: (10, 3)\n",
      "Loaded shape: (10, 3)\n",
      "Data preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Test different formats\n",
    "for format_type in [OutputFileType.CSV, OutputFileType.XLSX, OutputFileType.PARQUET]:\n",
    "    # Save data\n",
    "    saved_files, metadata = file_utils.save_with_metadata(\n",
    "        data={'data': df},\n",
    "        output_filetype=format_type,\n",
    "        output_type=\"processed\",\n",
    "        file_name=f\"format_test_{format_type.value}\"\n",
    "    )\n",
    "    \n",
    "    # Load and verify\n",
    "    loaded_data = file_utils.load_from_metadata(metadata)\n",
    "    print(f\"\\n{format_type.value.upper()} format test:\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Loaded shape: {loaded_data['data'].shape}\")\n",
    "    print(\"Data preserved:\", df.equals(loaded_data['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata Management\n",
    "\n",
    "Let's explore the metadata features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata contents:\n",
      "{\n",
      "  \"timestamp\": \"2025-10-19T01:12:22.940751\",\n",
      "  \"files\": {\n",
      "    \"raw\": {\n",
      "      \"path\": \"/Users/topi/data-science/FileUtils/data/processed/metadata_test_20251019_011222_raw.parquet\",\n",
      "      \"format\": \"parquet\"\n",
      "    },\n",
      "    \"processed\": {\n",
      "      \"path\": \"/Users/topi/data-science/FileUtils/data/processed/metadata_test_20251019_011222_processed.parquet\",\n",
      "      \"format\": \"parquet\"\n",
      "    }\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"directory_structure\": {\n",
      "      \"data\": [\n",
      "        \"raw\",\n",
      "        \"processed\"\n",
      "      ]\n",
      "    },\n",
      "    \"csv_delimiter\": \";\",\n",
      "    \"encoding\": \"utf-8\",\n",
      "    \"quoting\": 0,\n",
      "    \"include_timestamp\": true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save data with metadata\n",
    "saved_files, metadata = file_utils.save_with_metadata(\n",
    "    data={\n",
    "        'raw': df,\n",
    "        'processed': df.copy().assign(value=lambda x: x['value'] * 2)\n",
    "    },\n",
    "    output_filetype=OutputFileType.PARQUET,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"metadata_test\"\n",
    ")\n",
    "\n",
    "# Examine metadata contents\n",
    "with open(metadata, 'r') as f:\n",
    "    metadata_content = json.load(f)\n",
    "\n",
    "print(\"Metadata contents:\")\n",
    "print(json.dumps(metadata_content, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Azure Storage Integration\n",
    "\n",
    "To use Azure Storage, you'll need valid credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure setup not available: Azure connection string not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from FileUtils.core.base import StorageConnectionError\n",
    "\n",
    "# Load credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Try Azure connection\n",
    "try:\n",
    "    connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    if not connection_string:\n",
    "        raise ValueError(\"Azure connection string not found\")\n",
    "        \n",
    "    azure_utils = FileUtils(\n",
    "        storage_type=\"azure\",\n",
    "        connection_string=connection_string\n",
    "    )\n",
    "    \n",
    "    # Save to Azure\n",
    "    saved_files, metadata = azure_utils.save_with_metadata(\n",
    "        data={'test': df},\n",
    "        output_filetype=OutputFileType.PARQUET,\n",
    "        output_type=\"processed\",\n",
    "        file_name=\"azure_test\"\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully saved to Azure:\")\n",
    "    print(saved_files)\n",
    "    \n",
    "    # Load from Azure\n",
    "    loaded_data = azure_utils.load_from_metadata(metadata)\n",
    "    print(\"\\nSuccessfully loaded from Azure\")\n",
    "    \n",
    "except (ValueError, StorageConnectionError) as e:\n",
    "    print(f\"Azure setup not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Configuration\n",
    "\n",
    "Let's explore custom configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 01:12:23,030 - FileUtils.core.file_utils - INFO - Project root: /Users/topi/data-science/FileUtils\n",
      "2025-10-19 01:12:23,044 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "CSV with custom delimiter:\n",
      "date|value|category\n",
      "2024-01-01|0.4967141530112327|A\n",
      "2024-01-02|-0.13826430117118466|A\n",
      "2024-01-03|0.6476885381006925|B\n",
      "2024-01-04|1.5230298564080254|B\n",
      "2024-01-05|-0.23415337472333597|A\n",
      "2024-01-06|-0.23413695694918055|A\n",
      "2024-01-07|1.5792128155073915|A\n",
      "2024-01-08|0.7674347291529088|C\n",
      "2024-01-09|-0.4694743859349521|C\n",
      "2024-01-10|0.5425600435859647|C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Create custom config\n",
    "config = {\n",
    "    'csv_delimiter': '|',\n",
    "    'encoding': 'utf-8',\n",
    "    'include_timestamp': True,\n",
    "    'logging_level': 'DEBUG',\n",
    "    'directory_structure': {\n",
    "        'data': ['raw', 'interim', 'processed', 'external'],\n",
    "        'reports': ['figures', 'tables'],\n",
    "        'models': ['trained', 'evaluations']\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = Path('custom_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# Initialize with custom config\n",
    "custom_utils = FileUtils(config_file=config_path)\n",
    "\n",
    "# Test custom configuration\n",
    "saved_files, metadata = custom_utils.save_with_metadata(\n",
    "    data={'test': df},\n",
    "    output_filetype=OutputFileType.CSV,\n",
    "    output_type=\"processed\",\n",
    "    file_name=\"custom_config_test\"\n",
    ")\n",
    "\n",
    "# Show custom delimiter in action\n",
    "with open(list(saved_files.values())[0], 'r') as f:\n",
    "    print(\"CSV with custom delimiter:\")\n",
    "    print(f.read())\n",
    "\n",
    "# Clean up\n",
    "config_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "- Check the [Installation Guide](docs/INSTALLATION.md) for detailed setup instructions\n",
    "- See the [Usage Guide](docs/USAGE.md) for more examples and best practices\n",
    "- Explore the [Document Types Guide](docs/DOCUMENT_TYPES.md) for comprehensive document handling\n",
    "- Refer to [Azure Setup](docs/AZURE_SETUP.md) for cloud storage configuration\n",
    "\n",
    "For issues or suggestions, please visit the GitHub repository.\n",
    "\n",
    "For issues or suggestions, please visit the GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fileutils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
